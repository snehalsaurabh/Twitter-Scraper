{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP57GTv2Ia251S6ie+4CROD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Twitter News Scraper\n",
        "\n",
        "This notebook demonstrates how to scrape tweets from Twitter usernames using the ntscraper library with robust error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install ntscraper pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from ntscraper import Nitter\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scraper_function"
      },
      "outputs": [],
      "source": [
        "def scrape_twitter_users(usernames: List[str], tweets_per_user: int = 100, delay: int = 5) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Scrape tweets from specified Twitter usernames with robust error handling.\n",
        "    \n",
        "    Args:\n",
        "        usernames: List of Twitter usernames to scrape\n",
        "        tweets_per_user: Number of tweets to fetch per user\n",
        "        delay: Delay in seconds between requests\n",
        "    \n",
        "    Returns:\n",
        "        List of tweet dictionaries\n",
        "    \"\"\"\n",
        "    all_tweets = []\n",
        "    \n",
        "    # List of reliable Nitter instances (fallback options)\n",
        "    nitter_instances = [\n",
        "        \"https://nitter.net\",\n",
        "        \"https://nitter.privacydev.net\",\n",
        "        \"https://nitter.unixfox.eu\",\n",
        "        \"https://nitter.kavin.rocks\"\n",
        "    ]\n",
        "    \n",
        "    for username in usernames:\n",
        "        print(f\"\\n🔍 Scraping tweets from user: {username}\")\n",
        "        \n",
        "        # Try different instances if one fails\n",
        "        success = False\n",
        "        for i, instance in enumerate(nitter_instances):\n",
        "            try:\n",
        "                print(f\"  Trying instance {i+1}/{len(nitter_instances)}: {instance}\")\n",
        "                \n",
        "                # Initialize Nitter scraper\n",
        "                scraper = Nitter(log_level=1, skip_instance_check=True)\n",
        "                \n",
        "                # Get tweets for the current user\n",
        "                tweets = scraper.get_tweets(username, mode=\"user\", number=tweets_per_user, instance=instance)\n",
        "                \n",
        "                if tweets and 'tweets' in tweets and tweets['tweets']:\n",
        "                    all_tweets.extend(tweets['tweets'])\n",
        "                    print(f\"  ✅ Successfully scraped {len(tweets['tweets'])} tweets from {username}\")\n",
        "                    success = True\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"  ⚠️ No tweets found for {username} using {instance}\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Error with {instance}: {str(e)}\")\n",
        "                continue\n",
        "        \n",
        "        if not success:\n",
        "            print(f\"  💥 Failed to scrape tweets from {username} with all instances\")\n",
        "        \n",
        "        # Add delay between requests to be respectful\n",
        "        if username != usernames[-1]:  # Don't delay after the last user\n",
        "            print(f\"  ⏳ Waiting {delay} seconds before next request...\")\n",
        "            time.sleep(delay)\n",
        "    \n",
        "    return all_tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "configure_usernames"
      },
      "outputs": [],
      "source": [
        "# Configure the usernames to scrape\n",
        "usernames = [\n",
        "    'alanhenney',\n",
        "    'OCCRP', \n",
        "    'NOELreports',\n",
        "    'CBSEveningNews',\n",
        "    'CrimeChatt',\n",
        "    'phivolcs_dost'\n",
        "]\n",
        "\n",
        "print(f\"📋 Configured to scrape from {len(usernames)} users:\")\n",
        "for i, username in enumerate(usernames, 1):\n",
        "    print(f\"  {i}. @{username}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_scraper"
      },
      "outputs": [],
      "source": [
        "# Run the scraper\n",
        "print(\"🚀 Starting Twitter scraping process...\\n\")\n",
        "\n",
        "try:\n",
        "    # Scrape tweets\n",
        "    all_tweets = scrape_twitter_users(usernames, tweets_per_user=100, delay=5)\n",
        "    \n",
        "    print(f\"\\n📊 Scraping completed! Total tweets collected: {len(all_tweets)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"💥 An error occurred during the scraping process: {str(e)}\")\n",
        "    all_tweets = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "process_data"
      },
      "outputs": [],
      "source": [
        "# Process and save the data\n",
        "if all_tweets:\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(all_tweets)\n",
        "    \n",
        "    print(f\"📈 DataFrame created with shape: {df.shape}\")\n",
        "    print(f\"📋 Available columns: {list(df.columns)}\")\n",
        "    \n",
        "    # Display first few rows\n",
        "    print(\"\\n🔍 First 3 tweets:\")\n",
        "    display(df.head(3))\n",
        "    \n",
        "    # Save to CSV\n",
        "    output_file = \"dataset_crime_2.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    \n",
        "    print(f\"\\n💾 Data saved to {output_file}\")\n",
        "    \n",
        "    # Show some statistics\n",
        "    print(\"\\n📊 Data Statistics:\")\n",
        "    print(f\"  • Total tweets: {len(df)}\")\n",
        "    if 'user' in df.columns:\n",
        "        print(f\"  • Unique users: {df['user'].nunique()}\")\n",
        "        print(f\"  • Tweets per user:\")\n",
        "        user_counts = df['user'].value_counts()\n",
        "        for user, count in user_counts.items():\n",
        "            print(f\"    - {user}: {count} tweets\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ No tweets were scraped successfully. Please check the usernames and try again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_single_user"
      },
      "outputs": [],
      "source": [
        "# Test scraping a single user (for debugging)\n",
        "test_username = \"CBSEveningNews\"  # Change this to test different users\n",
        "\n",
        "print(f\"🧪 Testing single user scraping for: @{test_username}\")\n",
        "\n",
        "try:\n",
        "    test_tweets = scrape_twitter_users([test_username], tweets_per_user=10, delay=2)\n",
        "    \n",
        "    if test_tweets:\n",
        "        test_df = pd.DataFrame(test_tweets)\n",
        "        print(f\"✅ Test successful! Scraped {len(test_tweets)} tweets\")\n",
        "        print(f\"📋 Sample tweet data:\")\n",
        "        if len(test_tweets) > 0:\n",
        "            sample_tweet = test_tweets[0]\n",
        "            for key, value in sample_tweet.items():\n",
        "                print(f\"  {key}: {str(value)[:100]}{'...' if len(str(value)) > 100 else ''}\")\n",
        "    else:\n",
        "        print(\"❌ Test failed - no tweets scraped\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Test failed with error: {str(e)}\")"
      ]
    }
  ]
}